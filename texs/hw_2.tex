\documentclass[a4paper,12pt]{article}
\usepackage{amssymb}
\usepackage{dirtytalk}
\usepackage[mathcal]{euscript}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{float}
\graphicspath{{../imgs/hw2/}}
\begin{document}
\title{Домашнее задание №2}
\author{Дудырев Егор}
\date{\today}
\maketitle

\section{Задание}
\textbf{Задание E}\\
Программно построить по выбранной коллекции текстов две N-граммные модели - для N=2 и N=3, используя доступные средства (например, см. вариант D). Рассчитать по этим моделям и сравнить вероятности:
\begin{enumerate}
\item 5 фраз, присутствующих в исходной текстовой коллекции
\item 5 фраз, отсутствующих в ней, используя при этом один из методов сглаживания
\end{enumerate}
Рассчитать также перплексию - коэффициент неопределённости построенных моделей. Оценить и сравнить точность предсказания слов из указанных отсутствующих фраз построенными моделями.

Отчёт: Характеристика исходной текстовой коллекции (в том числе, как/откуда она получена), описание построенной языковой модели и метода её построения, проделанные расчёты, программа с комментариями, выводы.


\section{Текстовая коллекция}
В качестве коллекции текстов взяты 5 первых тома Полного собрания сочинений Л.Н. Толстого, а именно:
\begin{itemize}
\item \href{https://royallib.com/book/tolstoy_lev/polnoe_sobranie_sochineniy_tom_1_detstvo.html}{Том 1. Детство}
\item \href{https://royallib.com/book/tolstoy_lev/polnoe_sobranie_sochineniy_tom_2_yunost.html}{Том 2. Юность}
\item \href{https://royallib.com/book/tolstoy_lev/polnoe_sobranie_sochineniy_tom_3_proizvedeniya_18521856_gg.html}{Том 3. Произведения 1852-1856 гг.}
\item \href{https://royallib.com/book/tolstoy_lev/polnoe_sobranie_sochineniy_tom_4_proizvedeniya_sevastopolskogo_perioda_utro_pomeshchika.html}{Том 4. Произведения севастопольского периода. Утро помещика}
\item \href{https://royallib.com/book/tolstoy_lev/polnoe_sobranie_sochineniy_tom_5_proizvedeniya_18561859_gg.html}{Том 5. Произведения 1856-1859}
\end{itemize}
Томы 1-4 использовались для расчёта N-грамм. Том 5 использовался для расчёта перплексий.
В текстах, помимо собственно художественного произведения, содержится также различная побочная информация - ссылки, комментарии редакции и т.п.\\
Количество слов, на которых строились N-граммы - 346790.\\
Тестовые фразы:
\begin{itemize}
\item Фразы из текста
\begin{enumerate}
\item \say{Матушка сидела в гостиной и разливала чай}, Том1 детство
\item \say{И он ударил вилкой по столу}, Том 1 детство
\item \say{в карты не играл, кутил редко и курил простой табак}, Том 3. Произведения 1852
\item \say{Длинные чистые сакли с плоскими земляными крышами и красивыми трубами были расположены по неровным каменистым буграм, между которыми текла небольшая река}, Том 3. Произведения 1852
\item \say{Не видя никого в избе, Нехлюдов хотел уже выйти, как протяжный, влажный вздох изобличил хозяина} Том4 Прозведения севастопольского периода. утро помещика
\end{enumerate}
\item Фразы не из текста
\begin{enumerate}
\item \say{Но вот где является в полном блеске историческое воззрение г. Маркова}, Том 8. Педагогические статьи 1860
\item \say{Что же это такое понятие прогресса и вера в него}, Том8. Педагогические статьи 1860
\item \say{Высунувшееся из кареты лицо Наташи сияло насмешливою ласкою}, Том9. Война и мир
\item \say{Наполеон испытывал то несколько завистливое и беспокойное любопытство, которое испытывают люди при виде форм не знающей о них, чуждой жизни}, Том9. Война и мир
\item \say{Несколько купцов столпились около офицера}, Том9. Война и мир
\end{enumerate}
\end{itemize}



\section{Модели Sketch Engine}
Изначально для построения N-грамм был использован сервис Sketch Engine. Настройки и для 2N-грамм и для 3N-грамм следующие:
\begin{enumerate}
\item Отсутствие разделения на печатные и строчные символы
\item Аттрибут - слово
\item Минимальная частота - 0
\item Остальные значения по умолчанию
\end{enumerate}
Полученные модели указаны на рис. 1 и рис. 2.
\begin{figure}[H]
\caption{Биграммы, полученные с помощью Sketch Engine}
\centering
\includegraphics[width=\textwidth]{best_2ngrams}
\end{figure}
\begin{figure}[H]
\caption{Триграммы, полученные с помощью Sketch Engine}
\centering
\includegraphics[width=\textwidth]{best_3ngrams}
\end{figure}



\section{Собственные модели}
После расчёта моделей Sketch Engine оказалось, что даже при заданном параметре "Минимальная частота - 0", настоящая минимальная частота N-грамм в получившихся данных равна 3. Т.е., по каким-то причинам, Sketch Engine не сохранил редко употребляемые N-граммы, поэтому модель без сглаживания может указать вероятность фразы 0 даже на предложениях из обучающей текстовой коллекции.\\
В результате, N-граммы были расчитаны собственноручно на основе тех же данных, которые подавались в Sketch Engine.


\section{Результаты}
Для получения результатов рассчитаем вероятности фраз всеми имеющимися моделями (2-3 N-граммы, Sketch Engine и собственные моделия) для 10 равномерно распределённых (в логарифмическом масштабе) значений параметра alpha.\\
Получим следующие зависимости:
\begin{enumerate}
\item Рассчитанные вероятности для фраз из обучающего текста в целом выше, чем для фраз которых в тексте не было (рис. 3).

\begin{figure}[H]
\caption{Вероятности обучающих и тестовых фраз}
\centering
\includegraphics[width=\textwidth]{phrases_probs}
\end{figure}

\item Модели, учитывающие самые редкие N-граммы показывают сильнее разделяют фразы которые были в обучающем наборе от тех, которых не было (рис. 4).
\begin{figure}[H]
\caption{Вероятности обучающих и тестовых фраз в разрезе моделей}
\centering
\includegraphics[width=\textwidth]{phrases_probs1}
\end{figure}

\item При повышении коэффициента сглаживания вероятности фраз, которых не было в обучающем корпусе, повышаются. Фразы, которые были в корпусе, показывают стабильно высокую вероятность (рис. 5).
\begin{figure}[H]
\caption{Вероятности фраз при изменении коэффициента сглаживания}
\centering
\includegraphics[width=\textwidth]{phrases_prob_alpha}
\end{figure}

\item Рассмотрим только фразы из обучающего корпуса. Для моделей со всеми N-граммами вероятности фраз стабильно высокие. Для моделей без наиболее редких N-грамм вероятности возрастают с увеличением параметра сглаживания (рис. 6).
\begin{figure}[H]
\caption{Вероятности фраз из обучающей коллекции текстов}
\centering
\includegraphics[width=\textwidth]{phrases_prob_alpha_true}
\end{figure}

\item Для фраз не из обучающего корпуса вероятности повышаются с увеличением параметра сглаживания для всех моделей (Рис. 7).
\begin{figure}[H]
\caption{Вероятности фраз не из обучающей коллекции текстов}
\centering
\includegraphics[width=\textwidth]{phrases_prob_alpha_false}
\end{figure}

\end{enumerate}

Для расчёта перплексии моделей использовались предложения из пятого тома Полного собрания сочинения Л.Н. Толстого. Слова с 1000 по 1050. Предложения большей длины часто имели вероятность 0, что давало бесконечную перплексию.
\begin{figure}[H]
\caption{Перплексии моделей}
\centering
\includegraphics[width=\textwidth]{perplexions}
\end{figure}

Вывод: лучше всего для расчёта вероятности текста использовать все возможные N-граммы текста, даже если они встречаются всего один раз. С другой стороны, это может быть накладно с точки зрения используемых ресурсов. При невозможности хранить все возможные N-граммы необходимо использовать сглаживание со значением alpha близким к 1.


\section{Ссылки на код}
\begin{itemize}
\item \href{https://github.com/EgorDudyrev/hse_comp_ling/blob/master/notebooks/hw2/Looking_at_the_ngrams.ipynb}{Исполняемый код}
\end{itemize}

\end{document}